chu2018survey

A Survey of Domain Adaptation for Neural Machine Translation

Domain specific corpora is hard to find in machine translation.

Machine translation in general
Two major approaches: statistical machine translation (word alignments, translation rules, 
decoding mechanisms) and neural machine translation, the latter being the state of the art
at the moment.

Corpora is hard to find, for specific pairs of languages, or specific domains. Both vanilla
SMT and NMT perform badly for domain specific translation in low resource scenarios. 

Domain adaptation is leverages out-of-domain parallel corpora and in-domain monolingual corpora to
improve domain translation. For example, a corpus exists of patent data (one domain) for
English-Chinese translation containing 1M examples. But, for spoken data (second domain) a much smaller
dataset exists (200k sentences), thus methods to adapt patent data exist. Further, there exist 
monolingual resources for spoken data (second domain) which also serve to improve performance. These
are both domain adaptation techniques. 

For SMT, two main approaches exist for domain adaptation: data centric and model centric. Data
centric methods select data from out-of-domain parallel data based on a language model
or generating pseudo data. Model centric methods interpolate in-domain monolingual data
and out-of-domain models. 

Domain adaptation for NMT is a relatively new area. They mostly adopt ideas from SMT. Similar
to SMT, approaches for NMT domain adaptation are equivalently data and model centric. Data can
be used as in-domain monolingual corpora or parallel corpora or syntectic data, whereas
model centric approaches can shape the training objective, architecture or decoding algorithm. 

Neural Machine Translation
==========================

The encoder-decoder with attention is the most oft used NMT model. The model is also known as 
RNNsearch. The translation is generated as a probability or translation given the input 
and thus translated-part. As an objective function, the goal is to minimize the cross-entropy loss
of that probability. The RNN in the model is used to generate a representation of a list of words
given the word order. The GRUs encode and decode the sentences in both directions. The 
encoder and decoder and connected through an attention mechanism which computes a
weighted average of the recurrent representations generated by the encoder (works 
as a soft alignment). The attention vector along with the encoded representation
is passed on to the decoder which does a softmax to predict the next word. 

Domain Adaptation for SMT
=========================

Data centric approaches. 

1) With sufficient other-domain training data, ones scores on that out-of-domain 
data using in-domain models and selects data based on a cut-off of best scores. 
2) Generating pseudo-sentences using information retrieval, self-enhancing
or parallel word embeddings is another option when
not enough parallel corpora is available. 

Model centric approaches.

1) Combining models (LMs, translation models, etc.)
2) Instance level-interpolation: instance is scored using statistical methods, 
then SMT are trained by giving weight. 

Domain Adaptation for NMT
=========================

Data Centric.

1) Monolingual corpora 
- Gulchere train an RNNLM model on monolingual data and fuse
the RNNLM and NMT model 
- Currey (2017) copy the target monolingual data to the source
and retrain the model. 
- Zhang and Zong (2016) use source side
monolingual data to strenghten the NMT encoder via multitask learning
by predicting reordered sentences. 
- Cheng (2016) uses monolingual data
in an autoencoder setup

2) Synthetic Parallel Corpora

- Sennrich (2016) genrated syntactic data pairs using the decoder  
outputs

3) Out-of-Domain Parallel Corpora

- Chu (2017) developed a multi-domain approach which uses tags to control NMT
 The tags instruct the model that they are domain specific and the small
 corpus is oversampled from the specific domain
- Data selection does not work well in NMT, Van der Wees (2017) proposes
a dynamic data selection method where they gradually decrease the training 
data based on the in-domain similarty

Model Centric. 

1) Training Objective Centric

Instance cost/weighting
- Wang (2017) set a weight for the objective function 
to weigh between the in-domain and out-of-domain LM
- Chen (2017) employs a domain classifier which becomes the 
domain weight

Fine Tuning
- conventional way for domain adaptation: pre-train on out-of-domain
then fine tune on in-domain
- some more advanced methods keeps distribution based on knowledge
destilation (Hinton, 2015)

Mixed Fine Tuning
- combination of fine-tuning and multi-domain: train a model on
out-of-domain data until convergence, then fine-tune on a sample 
mix of in and out-of domain data (oversampling in-domain data) => I like 
- Chu (2017) showed that mixed fine tuning works better than both md and ft

Regularization
- Barone (2017) did standard L2 and dropout to prevent overfitting, and also
propose tuneout as a varient for dropout

2) Architecture Centric

Deep Fusion
- Train an in-domain RNNLM for NMT and combine it with a NMT model (fusion)
- Deep fusion creates a single decoder on the output, shallow fusion 
considers each individual score

Domain Discriminator
- Britz (2017) proposes a discriminative method with a feed-forward 
network (FFNN) as a discriminator on top of a encoder that uses 
attention to predict the domain, all optimized jointly => I like
 
Domain Control 
- Kobus (2016) proposes appending word-level features to the embedding layer, features
are tf-idf

3) Decoding Centric
Shallow Fusion 
- The next word is rescored by the weighted sum of NMT and RNNLM 
Ensampling 
- Freitag (2016) ensembles betwen the in-domain fine tuned and
out-of-domain models
Neural Lattice Search
- Khayrallay (2017) proposes a stack based decoding algorithm over word
lattices, which are generated by SMT

Domain Adaptation in Real-World Scenarios
=========================================

- readers are recommended to choose the method based on their situation
- example: both out-of-domain parallel and monolingual in-domain data are available, one
can combine several methods
- when the domain is not known, one usually builds domain-specific models (or uses tags)
and combines that with a domain classifier to first determine the domain
of the input text

Future directions
=================
- CNNs for NMT have been developed (much easier to optimize)
- Transformer methods (attention-only)
- Using domain-specific dictionaries (rare word translation)
- Cross-lingual transfer learning (out-of-domain in different language pairs)
- Adversarial Domain Adaptation and Domain generation, Britz (2017) did NMT using GANs


Questions raised:
- How does SMT look like exactly?
- How does "fusing" models in Data centric NMT monolingual work?
- What is knowledge destilation (Hinton, 2015)?
- WHat is tuneout (Barone, 2017)?
- What are word lattices (khayrallay, 2017

Papers to read after this one:
- Cho et. al: Learning Phrase Representations using RNN Encoderâ€“Decoder
for Statistical Machine Translation

